{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85fad2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import obonet,re,gzip,time,os\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "import requests\n",
    "import networkx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4315c1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obonet_modif(link):\n",
    "    file=obonet.read_obo(link,ignore_obsolete=False)\n",
    "    added_dict={}\n",
    "    for term in file.nodes:\n",
    "        attributes=file.nodes[term]##a dict\n",
    "        is_obsolete=attributes.get('is_obsolete','false')=='true'\n",
    "        if is_obsolete:##如果term已经obsolete，则直接给obsolete标签，否则给valid标签。因为在nodes中的点不会存在除这两种状态外的其他状态\n",
    "            attributes['status']='obsolete'\n",
    "        else:\n",
    "            attributes['status']='valid'\n",
    "        alter=attributes.get('alt_id',False)\n",
    "        if alter:\n",
    "            added_dict.update({x:term for x in alter})\n",
    "    file.add_nodes_from([(node,{'current':id,'status':'merged'}) for (node,id) in added_dict.items()])##给altered merged标签\n",
    "    \n",
    "    return file\n",
    "################find the url to corresponding version of GO and GOA#########################\n",
    "############################################################################################\n",
    "r=requests.get('https://go-data-product-release.s3.amazonaws.com/?list-type=2&delimiter=/&prefix=')\n",
    "pattern=re.compile('>[0-9]{4}\\-[0-9]{2}\\-[0-9]{2}/')\n",
    "all_date=pattern.findall(r.text)\n",
    "for x in all_date:\n",
    "    all_date.remove(x)\n",
    "    x=x.replace('>','')\n",
    "    x=x.replace('/','')\n",
    "    all_date.insert(0,x)\n",
    "all_date.sort()\n",
    "def corresponding_time(time):\n",
    "    annotation_online_path='http://release.geneontology.org/0000-00-00/annotations/goa_human.gaf.gz'\n",
    "    ontology_online_path='http://release.geneontology.org/0000-00-00/ontology/go.obo'\n",
    "    for x in all_date:\n",
    "        if time<x:\n",
    "            index=all_date.index(x)\n",
    "            break\n",
    "        else:\n",
    "            index=-1\n",
    "    if index==0:\n",
    "        time1=all_date[0]\n",
    "    else:\n",
    "        time1=all_date[index-1]\n",
    "    annotation_online_path = annotation_online_path.replace('0000-00-00', time1)\n",
    "    ontology_online_path = ontology_online_path.replace('0000-00-00', time1)\n",
    "    return [annotation_online_path, ontology_online_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6038f854",
   "metadata": {},
   "outputs": [],
   "source": [
    "class network:\n",
    "    def __init__(self,annotation='http://current.geneontology.org/annotations/goa_human.gaf.gz',ontology='http://purl.obolibrary.org/obo/go.obo',directed_only=True):\n",
    "        if re.match(r'^http:/{2}\\w.+$',annotation):\n",
    "#             file=requests.get(annotation)\n",
    "#             with open('annotation.gz','wb') as f:\n",
    "#                 f.write(file.content)\n",
    "#             f1 = gzip.open('annotation.gz', 'rb')\n",
    "#             f_out = open('text.txt', 'w')\n",
    "#             file_content = f1.read()\n",
    "#             f_out.write(file_content.decode('utf-8'))\n",
    "#             f.close()\n",
    "#             f_out.close()\n",
    "#             with open('text.txt', 'r') as out:\n",
    "#                 num = 0\n",
    "#                 for line1 in out.readlines():\n",
    "#                     if line1.startswith('!'):\n",
    "#                         num += 1\n",
    "#                         continue\n",
    "#                     else:\n",
    "#                         break\n",
    "#             annotation_f = pd.read_csv('text.txt', sep='\\t', skiprows=num, header=None,usecols=[1,3,4],names=['gene product','relationship','GO term'])\n",
    "            output=requests.get(annotation)\n",
    "            with open('annotation.gz','wb') as f:\n",
    "                f.write(output.content)\n",
    "            f1 = gzip.open('annotation.gz', 'rb')\n",
    "\n",
    "            items=[]\n",
    "            name=['gene product','relationship','GO term','evidence code']\n",
    "            for line in f1.readlines():\n",
    "                line=line.decode('utf-8')\n",
    "                if line.startswith('!')==False:\n",
    "                    elements=line.split('\\t')\n",
    "                    items.append((elements[1],elements[3],elements[4],elements[6]))\n",
    "            os.remove('annotation.gz')\n",
    "            annotation_f=pd.DataFrame.from_records(items,columns=name)\n",
    "        else:\n",
    "            with open(annotation, 'r') as out:\n",
    "                num = 0\n",
    "                for line1 in out.readlines():\n",
    "                    if line1.startswith('!'):\n",
    "                        num += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        break\n",
    "            annotation_f = pd.read_csv(annotation, sep='\\t', skiprows=num, header=None,usecols=[1,3,4,6],names=['gene product','relationship','GO term','evidence code'])\n",
    "        t1=time.time()\n",
    "        G_a = nx.from_pandas_edgelist(annotation_f, source='gene product', target='GO term', edge_key='relationship', create_using=nx.MultiDiGraph())\n",
    "        G_o = obonet.read_obo(ontology)\n",
    "        G_o_obsolete=obonet_modif(ontology)\n",
    "        annotation_gene=annotation_f[['gene product','GO term']].copy()\n",
    "        annotation_gene.drop_duplicates(inplace=True)\n",
    "        G_a_gene=nx.from_pandas_edgelist(annotation_gene,source='gene product',target='GO term',create_using=nx.MultiDiGraph())\n",
    "\n",
    "        if directed_only:\n",
    "            G_o.remove_edges_from([(x,y) for x,y,z in G_o.edges(keys=True) if z!='is_a'])\n",
    "        self.ontology=G_o\n",
    "        self.annotation=G_a\n",
    "        self.annotation_genes=G_a_gene\n",
    "        self.ontology_with_obsolete=G_o_obsolete\n",
    "\n",
    "    def categories(self):\n",
    "        G_o = self.ontology\n",
    "        bp = [x for x in G_o.nodes() if G_o.nodes[x]['namespace'] == 'biological_process']\n",
    "        mf = [x for x in G_o.nodes() if G_o.nodes[x]['namespace'] == 'molecular_function']\n",
    "        cc = [x for x in G_o.nodes() if G_o.nodes[x]['namespace'] == 'cellular_component']\n",
    "        return [bp, mf, cc]\n",
    "\n",
    "    def information_content_value(self,ignore_duplicates=False):\n",
    "        bp = self.categories()[0]\n",
    "        mf = self.categories()[1]\n",
    "        cc = self.categories()[2]\n",
    "        ontology = self.ontology\n",
    "        if ignore_duplicates==False:\n",
    "            annotation = self.annotation\n",
    "        else:\n",
    "            annotation=self.annotation_genes\n",
    "        IC_record_bp = {}\n",
    "        IC_record_mf = {}\n",
    "        IC_record_cc = {}\n",
    "\n",
    "        aspects = ['bp', 'mf', 'cc']\n",
    "        for aspect in aspects:\n",
    "            IC_file = eval('IC_record_' + aspect)\n",
    "            for term in eval(aspect):\n",
    "                try:\n",
    "                    child = [subterm for subterm in nx.ancestors(ontology, term)]\n",
    "                except networkx.exception.NetworkXError:\n",
    "                    IC_file[term] = len(annotation.in_edges(term))\n",
    "\n",
    "                else:\n",
    "                    child.append(term)\n",
    "                    ICs = np.sum([len(annotation.in_edges(child_term)) for child_term in child])\n",
    "                    IC_file[term]=ICs  # 这一步用于计算information content\n",
    "        return [IC_record_bp, IC_record_mf, IC_record_cc]\n",
    "    \n",
    "class networks(network):\n",
    "    def __init__(self,annotation,ontology,directed_only=True,ignore_duplicates=False,aspect='biological_process'):\n",
    "        super().__init__(annotation,ontology,directed_only)\n",
    "        self.IC_values=super().information_content_value(ignore_duplicates)\n",
    "        self.aspect=aspect\n",
    "####here, the MICA is based on IC values.         \n",
    "    def MICA(self,a,b,return_type='ID',subterms=None):\n",
    "        root_list=['GO:0008150','GO:0003674','GO:0005575']\n",
    "        namespace=['biological_process','molecular_function','cellular_component']\n",
    "        aspect=self.aspect\n",
    "        IC_values=self.IC_values\n",
    "                ##对于正常情况\n",
    "        if subterms:\n",
    "            ontology=self.ontology.subgraph(subterms)\n",
    "        else:\n",
    "            ontology=self.ontology\n",
    "        idx_a=namespace.index(ontology.nodes[a]['namespace'])\n",
    "        idx_b=namespace.index(ontology.nodes[b]['namespace'])\n",
    "        ##解决特殊情况\n",
    "        if a==b:\n",
    "            if return_type=='IC':\n",
    "                return IC_values[idx_a][a]\n",
    "            else:\n",
    "                return a\n",
    "        if a in root_list:\n",
    "            if return_type=='IC':\n",
    "                return IC_values[idx_a][a]\n",
    "            else:\n",
    "                return a\n",
    "        if b in root_list:\n",
    "            if return_type=='IC':\n",
    "                return IC_values[idx_b][b]\n",
    "            else:\n",
    "                return b\n",
    "        \n",
    "\n",
    "        \n",
    "        parentA = [x for x in nx.descendants(ontology, a)]\n",
    "        parentA.append(a)\n",
    "        parentB = [x for x in nx.descendants(ontology, b)]\n",
    "        parentB.append(b)\n",
    "        common_ancestors = set(parentA) & set(parentB)\n",
    "        \n",
    "        mica_value=np.inf\n",
    "        mica=''\n",
    "        for x in common_ancestors:\n",
    "            idx1=namespace.index(aspect)\n",
    "            x_score=IC_values[idx1][x]\n",
    "            if x_score<mica_value:\n",
    "                mica=x\n",
    "                mica_value=x_score\n",
    "            else:\n",
    "                continue\n",
    "        if return_type=='IC':\n",
    "            return mica_value\n",
    "        else:\n",
    "            return mica\n",
    "    def similarity_score_position(self,image=False):\n",
    "        aspects=['biological_process','molecular_function','cellular_component']\n",
    "        aspect=self.aspect\n",
    "        idx=aspects.index(aspect)\n",
    "        IC=self.IC_values[idx]\n",
    "        terms=list(IC.keys())\n",
    "        denominator=max(IC.values())\n",
    "        maxp=-np.log(1/(denominator+0.001))\n",
    "        network1=self.ontology.subgraph(terms)\n",
    "        IC_non_zero_id=[x for x,y in IC.items() if y!=0]\n",
    "        have_d=[x for x in IC_non_zero_id if len(nx.ancestors(network1,x))>2]\n",
    "        for term in have_d:\n",
    "            if IC[term]>50:\n",
    "                continue\n",
    "            n=0\n",
    "            descendants=nx.ancestors(network1,term)\n",
    "            for de in descendants:\n",
    "                if IC[de]==0:\n",
    "                    continue\n",
    "                else:\n",
    "                    n+=1\n",
    "            if n<2:\n",
    "                have_d.remove(term)\n",
    "        IC_non_zero=[y for x,y in IC.items() if y!=0 and x in have_d]\n",
    "        x0=[(-np.log(x/(denominator+0.001)))/maxp for x in IC_non_zero]\n",
    "        x1=Counter(x0)\n",
    "        if image==True:\n",
    "            fx=dict(sorted(x1.items()))\n",
    "            fig,ax=plt.subplots()\n",
    "            f1=ax.scatter(fx.keys(),fx.values())\n",
    "            plt.show()\n",
    "        return x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02c3d840",
   "metadata": {},
   "outputs": [],
   "source": [
    "class similarity:\n",
    "    def __init__(self,endtime,starttime=None,directed_only=True,ignore_duplicates=False,aspect='biological_process'):#time should be in yyyy-mm-dd format\n",
    "\n",
    "        if starttime==None:\n",
    "            info2=info1=corresponding_time(endtime)\n",
    "        else:\n",
    "            info1=corresponding_time(starttime)\n",
    "            info2=corresponding_time(endtime)\n",
    "        #self.start_IC=networks(info1[0],info1[1],directed_only=True)\n",
    "        self.end_IC=networks(info2[0],info2[1],directed_only,ignore_duplicates,aspect)\n",
    "        self.score_evaluate=self.end_IC.similarity_score_position()\n",
    "        self.aspect=aspect\n",
    "    def set_similarity(self, seta, setb): #,only_interested=None,):\n",
    "        namespace=['biological_process','molecular_function','cellular_component']\n",
    "        if type(seta).__name__ != 'list':\n",
    "            if seta.endswith('.txt'):\n",
    "                sa = pd.read_csv(seta, sep='\\t', header=None, names=['GO'])\n",
    "                seta = sa['GO'].to_list()\n",
    "            else:\n",
    "                return 'wrong input'\n",
    "        if type(setb).__name__ != 'list':\n",
    "            if setb.endswith('.txt'):\n",
    "                sb = pd.read_csv(setb, sep='\\t', header=None, names=['GO'])\n",
    "                setb = sb['GO'].to_list()\n",
    "            else:\n",
    "                return 'wrong input'\n",
    "        aspect=self.aspect\n",
    "        e_ont=self.end_IC.ontology\n",
    "        e_net=self.end_IC\n",
    "        IC_values=e_net.IC_values#information_content_value()   \n",
    "        e_obs_nodes=e_net.ontology_with_obsolete.nodes\n",
    "        if seta==[] or setb==[]:\n",
    "            return 'empty input'\n",
    "        setA=[]\n",
    "        for term in list(seta): \n",
    "            attribute=e_obs_nodes[term]\n",
    "            status=attribute['status']\n",
    "            if status=='valid':\n",
    "                setA.append(term)\n",
    "            elif status=='merged': ##将merged term换成其alternative terms\n",
    "                sub=attribute['current']\n",
    "                setA.append(sub)\n",
    "            else:                  ##对于obsolete term,查看其是否有replace_by 或者 consider，如果都没有，那么直接删除\n",
    "                replaceby=attribute.get('replaced_by',[])\n",
    "                consider=attribute.get('consider',[])\n",
    "                a1=[x for x in replaceby]\n",
    "                a2=[x for x in consider]\n",
    "                setA.extend(a1)\n",
    "                setA.extend(a2)\n",
    "\n",
    "        setB=[]\n",
    "        for term in list(setb): \n",
    "            attribute=e_obs_nodes[term]\n",
    "            status=attribute['status']\n",
    "            if status=='valid':\n",
    "                setB.append(term)\n",
    "            elif status=='merged': ##将merged term换成其alternative terms\n",
    "                sub=attribute['current']\n",
    "                setB.append(sub)\n",
    "            else:                  ##对于obsolete term,查看其是否有replace_by 或者 consider，如果都没有，那么直接删除\n",
    "                replaceby=attribute.get('replaced_by',[])\n",
    "                consider=attribute.get('consider',[])\n",
    "                a1=[x for x in replaceby]\n",
    "                a2=[x for x in consider]\n",
    "                setB.extend(a1)\n",
    "                setB.extend(a2)\n",
    "        setA=[x for x in setA if e_ont.nodes[x]['namespace']==aspect]\n",
    "        setb=[x for x in setB if e_ont.nodes[x]['namespace']==aspect]\n",
    "        if setb==[] or setA==[]:\n",
    "            return 'Unrelated'\n",
    "        onlyA=set(setA)-set(setb)\n",
    "        onlyB=set(setb)-set(setA)\n",
    "        common_set=set(setA)&set(setb)\n",
    "        IC_value=IC_values[namespace.index(aspect)]\n",
    "        max_value=max(IC_value.values())\n",
    "        max_score=-np.log(1/max_value)\n",
    "        #scores=sum([-np.log(IC_value[x]/max_value)/max_score for x in common_set])*2\n",
    "        scores=len(common_set)*2#*(len(common_set)/len(set(setA)|set(setb)))\n",
    "        for term1 in onlyA:\n",
    "            value=min([e_net.MICA(term1,x,'IC') for x in setb])\n",
    "            value=-np.log(value/max_value)\n",
    "            value_norm=value/max_score\n",
    "            scores+=value_norm\n",
    "        for term2 in onlyB:\n",
    "            value=min([e_net.MICA(term2,x,'IC') for x in setA])\n",
    "            value=-np.log(value/max_value)\n",
    "            value_norm=value/max_score\n",
    "            scores+=value_norm\n",
    "        final_score=scores/(len(setA)+len(setb))\n",
    "        print(final_score)\n",
    "        score_quantile=self.score_evaluate\n",
    "        #score_quantile=list(set(score_quantile))\n",
    "        intervals=np.percentile(score_quantile,[30,60,80])\n",
    "        if final_score==1:\n",
    "            return 'Identical'\n",
    "        if final_score<=intervals[0]:\n",
    "            return 'Unrelated'\n",
    "        if final_score>intervals[0] and final_score<=intervals[1]:\n",
    "            return 'Related'\n",
    "        if final_score>intervals[1] and final_score<=intervals[2]:\n",
    "            return 'Similar'\n",
    "        else:\n",
    "            return 'Compatible'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba05861",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
